# -*- coding: utf-8 -*-
"""DS-Putu Adelia Devani Ardiana.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nl9qJem0XtsQ_aTXx0kdogPTfDWYK_aa
"""

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi


#DATA PREPROCESSING
#Mengimpor dataset
df = pd.read_csv("arxiv_ml.csv")

#NLTK
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt_tab', quiet=True)

#Data Preprocessing (menghilangkan yang tidak dibutuhkan)
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

#Menggunakan sample dikarenakan jumlah dataset yang besar
df = df.sample(5000, random_state=42) if len(df) > 5000 else df
df["processed_abstract"] = df["abstract"].apply(preprocess_text)

#Fungsi untuk chunking
def chunk_abstract(abstract, chunk_size=3):
    sentences = sent_tokenize(abstract)
    return [" ".join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]

df["chunks"] = df["abstract"].apply(chunk_abstract)


# RETRIEVAL DENGAN BM25
# Menggunakan BM25 untuk retrieval
tokenized_abstracts = df["processed_abstract"].tolist()
bm25 = BM25Okapi([text.split() for text in tokenized_abstracts])

def retrieve_top_k(query, bm25, df, k=5):
    query_tokens = preprocess_text(query).split()
    scores = bm25.get_scores(query_tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
    return df.iloc[top_indices]


#GENERASI JAWABAN DENGAN MODEL T5
#Generasi dengan T5
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

def generate_answer(question, df):
    combined_context = " ".join(df["chunks"].sum())
    input_text = f"question: {question} context: {combined_context}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=512)
    output_ids = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)


#RELEVANCY RESPONSE
#Menghitung response relevancy dengan menggunakan transformers
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def response_relevancy_score(answer, context):
    answer_embedding = embedding_model.encode(answer, convert_to_tensor=True)
    context_embedding = embedding_model.encode(context, convert_to_tensor=True)
    similarity_score = cosine_similarity(answer_embedding.reshape(1, -1), context_embedding.reshape(1, -1))
    return similarity_score[0][0]


#CONTEXT PRECISION
#Menghitung context precision
def context_precision_score(retrieved_context, reference_contexts):
    #Menghitung kemiripan antara konteks yang diambil dan setiap konteks referensi
    similarities = []
    retrieved_embedding = embedding_model.encode(retrieved_context, convert_to_tensor=True)

    for ref_context in reference_contexts:
        reference_embedding = embedding_model.encode(ref_context, convert_to_tensor=True)
        similarity = cosine_similarity(retrieved_embedding.reshape(1, -1), reference_embedding.reshape(1, -1))
        similarities.append(similarity[0][0])

    #Mengkalkulasi score untuk context precision
    return sum(similarities) / len(similarities) if similarities else 0


#GROUND TRUTH
#Pemilihan GT secara manual

#Pertanyaan 1
ground_truth = { "What is machine learning?": "Machine learning is a branch of artificial intelligence (AI) that allows computer systems to learn and evolve independently. ML uses data and algorithms to mimic the way humans learn. "}

#Pertanyaan 2
#ground_truth = {"What are the recent advances in asynchronous parallel and distributed optimization methods for machine learning?":"There have been several advances in the study of asynchronous parallel and distributed optimization methods during the past decade. Asynchronous methods do not require all processors to maintain a consistent view of the optimization variables. Consequently, they generally can make more efficient use of computational resources than synchronous methods, and they are not sensitive to issues like stragglers (i.e., slow nodes) and unreliable communication links. Mathematical modeling of asynchronous methods involves proper accounting of information delays, which makes their analysis challenging. This article reviews recent developments in the design and analysis of asynchronous optimization methods, covering both centralized methods, where all processors update a master copy of the optimization variables, and decentralized methods, where each processor maintains a local copy of the variables. The analysis provides insights as to how the degree of asynchrony impacts convergence rates, especially in stochastic optimization methods."}

#Pertanyaan 3
#ground_truth = { "How does the new approach for learning treewidth-bounded Bayesian Networks scale to large networks?": "The key to our approach is applying an exact method (based on MaxSAT) locally, to improve the score of a heuristically computed BN."}


#Pertanyaan 4
#ground_truth = { "How does the deep learning model predict crop yield using genotype and weather data, and how does it compare to traditional methods?": "Accurate prediction of crop yield supported by scientific and domain-relevant insights, can help improve agricultural breeding, provide monitoring across diverse climatic conditions and thereby protect against climatic challenges to crop production including erratic rainfall and temperature variations."}


#Pertanyaan 5
#ground_truth = { "What are the advantages and challenges of using LSTM and ARIMA models for forecasting COVID-19 cases?": " LSTM models slightly underestimated while ARIMA slightly overestimated the numbers in the forecasts. The performance of LSTM models is comparable to ARIMA in forecasting COVID-19 cases. While ARIMA requires longer sequences, LSTMs can perform reasonably well with sequence sizes as small as 3. "}

#Fungsi untuk mengevaluasi jawaban dengan GT
def evaluate_answer(answer, question):
    gt = ground_truth.get(question, None)

    if gt:
        #Mengkalkulasikan persamaan antara jawaban dan GT
        score = response_relevancy_score(answer, gt)
        print("GT found for this question")
    else:
        print("GT not found for this question")


#INTERFACE
#Input pertanyaan
text = "QA MACHINE LEARNING! :D"
title_text = text.title()
print(title_text)

question = input("Enter your question: ")
retrieved_docs = retrieve_top_k(question, bm25, df, k=5)
answer = generate_answer(question, retrieved_docs)

#Menggabungkan semua konteks jawaban untuk penilaian relevansi
context_text = " ".join(retrieved_docs["chunks"].sum())

#Mengkalkulasikan response relevancy
response_score = response_relevancy_score(answer, context_text)

#Menggunakan dokumen teratas yang diambil sebagai konteks referensi
reference_contexts = [" ".join(doc) for doc in retrieved_docs["chunks"]]

#Print out jawaban dan score
precision_score = context_precision_score(context_text, reference_contexts)

print("\nAnswer: ", answer)
print("Response Relevancy Score: ", response_score)
print("Context Precision Score: ", precision_score)

#Perbandingan dengan GT
evaluate_answer(answer, question)
print("Ground Truth: ", ground_truth)

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import T5Tokenizer, T5ForConditionalGeneration
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi


#DATA PREPROCESSING
#Mengimpor dataset
df = pd.read_csv("arxiv_ml.csv")

#NLTK
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt_tab', quiet=True)

#Data Preprocessing (menghilangkan yang tidak dibutuhkan)
def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

#Menggunakan sample dikarenakan jumlah dataset yang besar
df = df.sample(5000, random_state=42) if len(df) > 5000 else df
df["processed_abstract"] = df["abstract"].apply(preprocess_text)

#Fungsi untuk chunking
def chunk_abstract(abstract, chunk_size=3):
    sentences = sent_tokenize(abstract)
    return [" ".join(sentences[i:i + chunk_size]) for i in range(0, len(sentences), chunk_size)]

df["chunks"] = df["abstract"].apply(chunk_abstract)


# RETRIEVAL DENGAN BM25
# Menggunakan BM25 untuk retrieval
tokenized_abstracts = df["processed_abstract"].tolist()
bm25 = BM25Okapi([text.split() for text in tokenized_abstracts])

def retrieve_top_k(query, bm25, df, k=5):
    query_tokens = preprocess_text(query).split()
    scores = bm25.get_scores(query_tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
    return df.iloc[top_indices]


#GENERASI JAWABAN DENGAN MODEL T5
#Generasi dengan T5
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

def generate_answer(question, df):
    combined_context = " ".join(df["chunks"].sum())
    input_text = f"question: {question} context: {combined_context}"
    input_ids = tokenizer.encode(input_text, return_tensors="pt", truncation=True, max_length=512)
    output_ids = model.generate(input_ids, max_length=100, num_beams=5, early_stopping=True)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)


#RELEVANCY RESPONSE
#Menghitung response relevancy dengan menggunakan transformers
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def response_relevancy_score(answer, context):
    answer_embedding = embedding_model.encode(answer, convert_to_tensor=True)
    context_embedding = embedding_model.encode(context, convert_to_tensor=True)
    similarity_score = cosine_similarity(answer_embedding.reshape(1, -1), context_embedding.reshape(1, -1))
    return similarity_score[0][0]


#CONTEXT PRECISION
#Menghitung context precision
def context_precision_score(retrieved_context, reference_contexts):
    #Menghitung kemiripan antara konteks yang diambil dan setiap konteks referensi
    similarities = []
    retrieved_embedding = embedding_model.encode(retrieved_context, convert_to_tensor=True)

    for ref_context in reference_contexts:
        reference_embedding = embedding_model.encode(ref_context, convert_to_tensor=True)
        similarity = cosine_similarity(retrieved_embedding.reshape(1, -1), reference_embedding.reshape(1, -1))
        similarities.append(similarity[0][0])

    #Mengkalkulasi score untuk context precision
    return sum(similarities) / len(similarities) if similarities else 0


#INTERFACE
#Input pertanyaan
text = "QA MACHINE LEARNING! :D"
title_text = text.title()
print(title_text)

question = input("Enter your question: ")
retrieved_docs = retrieve_top_k(question, bm25, df, k=5)
answer = generate_answer(question, retrieved_docs)

#Menggabungkan semua konteks jawaban untuk penilaian relevansi
context_text = " ".join(retrieved_docs["chunks"].sum())

#Mengkalkulasikan response relevancy
response_score = response_relevancy_score(answer, context_text)

#Menggunakan dokumen teratas yang diambil sebagai konteks referensi
reference_contexts = [" ".join(doc) for doc in retrieved_docs["chunks"]]

#Print out jawaban dan score
precision_score = context_precision_score(context_text, reference_contexts)

print("\nAnswer: ", answer)
print("Response Relevancy Score: ", response_score)
print("Context Precision Score: ", precision_score)